{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LSTM-SAC: LSTM-based Soft Actor-Critic for Cancer Chemotherapy (POMDP)\n",
        "=======================================================================\n",
        "\n",
        "This is a single-file, self-contained implementation of Recurrent SAC using\n",
        "LSTM-based recurrent networks, following the patterns from the repository.\n",
        "\n",
        "Uses the same RNNBase/ContextualModel patterns as the repository but with\n",
        "layer_type='lstm' instead of 'gru'.\n",
        "\n",
        "Features:\n",
        "- LSTM-based recurrent actor and critic networks (using torch.nn.LSTM)\n",
        "- Repository-style RNNHidden container with LSTM (h,c) tuple support\n",
        "- Full-trajectory replay buffer with burn-in support\n",
        "- Automatic entropy tuning\n",
        "- Cancer chemotherapy (AhnChemoEnv) environment included\n",
        "\n",
        "Requirements: torch, numpy, gymnasium, scipy, matplotlib, pandas\n",
        "\n",
        "Author: LSTM-SAC Implementation following repository patterns\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from collections import namedtuple, OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple, Union, Any, TypeVar\n",
        "from abc import abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from scipy.integrate import solve_ivp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    HAS_SEABORN = True\n",
        "except ImportError:\n",
        "    HAS_SEABORN = False\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 1: AhnChemoEnv Environment (Cancer Chemotherapy)\n",
        "# ==============================================================================\n",
        "\n",
        "ObsType = TypeVar(\"ObsType\")\n",
        "\n",
        "\n",
        "def to_scalar(x):\n",
        "    \"\"\"Convert numpy array to scalar.\"\"\"\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return float(x.item())\n",
        "    return float(x)\n",
        "\n",
        "\n",
        "class BaseSimulator:\n",
        "    def __init__(self): pass\n",
        "    @abstractmethod\n",
        "    def activate(self) -> OrderedDict[str, np.ndarray]: raise NotImplementedError\n",
        "    @abstractmethod\n",
        "    def update(self, action: Union[dict, float], state: dict) -> OrderedDict[str, np.ndarray]: raise NotImplementedError\n",
        "\n",
        "\n",
        "class BaseReward:\n",
        "    def __init__(self): pass\n",
        "    @abstractmethod\n",
        "    def count_reward(self, *args, **kwargs) -> float: raise NotImplementedError\n",
        "\n",
        "\n",
        "def uniform_random(mean, width, absolute=False):\n",
        "    \"\"\"Generate uniform random value around mean.\"\"\"\n",
        "    def single_random(mean, width):\n",
        "        if absolute:\n",
        "            return float(np.random.uniform(mean - width, mean + width))\n",
        "        return float(np.random.uniform(mean - mean * width, mean + mean * width))\n",
        "    if isinstance(mean, (list, np.ndarray)):\n",
        "        return [single_random(m, width) for m in mean]\n",
        "    return single_random(mean, width)\n",
        "\n",
        "\n",
        "class AhnReward(BaseReward):\n",
        "    \"\"\"Reward function for cancer chemotherapy optimization.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def count_reward(self, state, init_state, action, terminated) -> float:\n",
        "        N, T, I, B = state[\"N\"], state[\"T\"], state[\"I\"], state[\"B\"]\n",
        "        N0, T0 = init_state[\"N\"], init_state[\"T\"]\n",
        "        reward = (N / N0) - (T / T0) - action\n",
        "        if terminated:\n",
        "            return -100.0\n",
        "        return to_scalar(reward)\n",
        "\n",
        "\n",
        "class AhnODE(BaseSimulator):\n",
        "    \"\"\"ODE simulator for cancer cell dynamics with chemotherapy.\"\"\"\n",
        "    def __init__(self, state_noise, pkpd_noise):\n",
        "        super().__init__()\n",
        "        self.state_noise = state_noise\n",
        "        self.pkpd_noise = pkpd_noise\n",
        "        self.cur_time = 0\n",
        "        self.time_interv = 0.25\n",
        "\n",
        "    def activate(self) -> OrderedDict[str, np.ndarray]:\n",
        "        width = self.pkpd_noise * 0.5\n",
        "        self.r2, self.b2, self.c4, self.a3 = (\n",
        "            uniform_random(1., width), uniform_random(1., width),\n",
        "            uniform_random(1., width), uniform_random(0.1, width)\n",
        "        )\n",
        "        self.r1, self.b1, self.c2, self.c3, self.a2 = (\n",
        "            uniform_random(1.5, width), uniform_random(1., width),\n",
        "            uniform_random(0.5, width), uniform_random(1., width), uniform_random(0.3, width)\n",
        "        )\n",
        "        self.s, self.rho, self.alpha, self.c1, self.d1, self.a1 = (\n",
        "            uniform_random(0.33, width), uniform_random(0.01, width),\n",
        "            uniform_random(0.3, width), uniform_random(1., width),\n",
        "            uniform_random(0.2, width), uniform_random(0.2, width)\n",
        "        )\n",
        "        self.d2 = uniform_random(1., width)\n",
        "        init_state = OrderedDict({\n",
        "            \"N\": np.array([uniform_random(0.9, self.state_noise * 0.5)], dtype=np.float32),\n",
        "            \"T\": np.array([uniform_random(0.2, self.state_noise * 0.5)], dtype=np.float32),\n",
        "            \"I\": np.array([uniform_random(0.005, self.state_noise * 0.5)], dtype=np.float32),\n",
        "            \"B\": np.array([0.0], dtype=np.float32)\n",
        "        })\n",
        "        self.cur_time = 0\n",
        "        return init_state\n",
        "\n",
        "    def update(self, action, state):\n",
        "        def odes_fn(t, variables, u):\n",
        "            N, T, I, B = variables\n",
        "            dNdt = self.r2 * N * (1 - self.b2 * N) - self.c4 * T * N - self.a3 * (1 - np.exp(-B)) * N\n",
        "            dTdt = self.r1 * T * (1 - self.b1 * T) - self.c2 * I * T - self.c3 * T * N - self.a2 * (1 - np.exp(-B)) * T\n",
        "            dIdt = self.s + self.rho * I * T / (self.alpha + T) - self.c1 * I * T - self.d1 * I - self.a1 * (1 - np.exp(-B)) * I\n",
        "            dBdt = -self.d2 * B + u\n",
        "            if self.state_noise > 0:\n",
        "                noise = np.random.normal(0, self.state_noise, 4)\n",
        "                dNdt += dNdt * noise[0]\n",
        "                dTdt += dTdt * noise[1]\n",
        "                dIdt += dIdt * noise[2]\n",
        "                dBdt += dBdt * noise[3]\n",
        "            return [dNdt, dTdt, dIdt, dBdt]\n",
        "\n",
        "        variables = np.array((state[\"N\"], state[\"T\"], state[\"I\"], state[\"B\"])).flatten()\n",
        "        scalar_action = to_scalar(action)\n",
        "        solution = solve_ivp(odes_fn, (self.cur_time, self.cur_time + self.time_interv),\n",
        "                            variables, args=(scalar_action,))\n",
        "        N, T, I, B = [max(0, solution.y[i, -1]) for i in range(4)]\n",
        "        self.cur_time += self.time_interv\n",
        "        return OrderedDict({\n",
        "            \"N\": np.array([N], dtype=np.float32),\n",
        "            \"T\": np.array([T], dtype=np.float32),\n",
        "            \"I\": np.array([I], dtype=np.float32),\n",
        "            \"B\": np.array([B], dtype=np.float32)\n",
        "        })\n",
        "\n",
        "\n",
        "class AhnChemoEnv(gym.Env):\n",
        "    \"\"\"Cancer Chemotherapy Environment (POMDP).\"\"\"\n",
        "    def __init__(self, max_t=600, obs_noise=0.2, state_noise=0.5, pkpd_noise=0.1,\n",
        "                 missing_rate=0.0, **kwargs):\n",
        "        super().__init__()\n",
        "        self.Simulator = AhnODE(state_noise=state_noise, pkpd_noise=pkpd_noise)\n",
        "        self.Reward = AhnReward()\n",
        "        self.obs_noise = obs_noise\n",
        "        self.missing_rate = missing_rate\n",
        "        self.max_t = max_t\n",
        "        self.t = 0\n",
        "        self.observation_space = spaces.Box(low=0.0, high=2.0, shape=(3,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "        self.state_map = {\"N\": \"Normal Cells\", \"T\": \"Tumor Cells\", \"I\": \"Immune Cells\", \"B\": \"Drug Concentration\"}\n",
        "\n",
        "    def _state2obs(self, state, enable_missing):\n",
        "        obs = np.array([state['T'], state['I'], state[\"B\"]]).flatten()\n",
        "        obs += self.obs_noise * obs * np.random.uniform(-0.5, 0.5, size=obs.shape)\n",
        "        obs = np.clip(obs, self.observation_space.low, self.observation_space.high).astype(np.float32)\n",
        "        if enable_missing and np.random.uniform(0, 1) < self.missing_rate:\n",
        "            return self.last_obs\n",
        "        self.last_obs = obs\n",
        "        return obs\n",
        "\n",
        "    def reset(self, seed=None, **kwargs):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.t = 0\n",
        "        self.init_state = self.Simulator.activate()\n",
        "        self.cur_state = self.init_state\n",
        "        obs = self._state2obs(self.init_state, False)\n",
        "        info = {\"state\": {self.state_map[k]: to_scalar(v) for k, v in self.init_state.items()}}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        action_scalar = np.clip(to_scalar(action), 0.0, 1.0)\n",
        "        state_next = self.Simulator.update(action=np.array([action_scalar]), state=self.cur_state)\n",
        "        obs_next = self._state2obs(state_next, True)\n",
        "        terminated = to_scalar(state_next[\"N\"]) < to_scalar(self.init_state[\"N\"]) * 0.7\n",
        "        truncated = self.t + 1 >= self.max_t\n",
        "        reward = self.Reward.count_reward(self.cur_state, self.init_state, action_scalar, terminated)\n",
        "        self.t += 1\n",
        "        self.cur_state = state_next\n",
        "        info = {\"state\": {self.state_map[k]: to_scalar(v) for k, v in state_next.items()}}\n",
        "        return obs_next, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "def create_AhnChemoEnv_setting1():\n",
        "    \"\"\"Create AhnChemoEnv with Setting 1 (lower noise).\"\"\"\n",
        "    return AhnChemoEnv(obs_noise=0.5, state_noise=0.0, pkpd_noise=0.0)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: RNNHidden Container (From Repository - with LSTM support)\n",
        "# ==============================================================================\n",
        "\n",
        "class RNNHidden:\n",
        "    \"\"\"\n",
        "    Container for managing RNN hidden states across multiple recurrent layers.\n",
        "\n",
        "    Supports both GRU (single tensor) and LSTM (tuple of h,c tensors).\n",
        "    This follows the repository's RNNHidden pattern exactly.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rnn_num: int, rnn_types: List[str],\n",
        "                 device: torch.device = torch.device('cpu'), batch_first: bool = False):\n",
        "        self._rnn_types = copy.deepcopy(rnn_types)\n",
        "        self._rnn_num = rnn_num\n",
        "        self._data: List[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]] = []\n",
        "        self._device = device\n",
        "        self._batch_first = batch_first\n",
        "\n",
        "    def append(self, hidden_state: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]) -> None:\n",
        "        \"\"\"Append hidden state. For LSTM, expects (h, c) tuple.\"\"\"\n",
        "        assert len(self._data) < self._rnn_num\n",
        "        self._data.append(hidden_state)\n",
        "\n",
        "    def __getitem__(self, key) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor], \"RNNHidden\"]:\n",
        "        if isinstance(key, slice):\n",
        "            rnn_types = self._rnn_types[key]\n",
        "            data = self._data[key]\n",
        "            result = RNNHidden(len(data), rnn_types, self._device, self._batch_first)\n",
        "            result._data = data\n",
        "            return result\n",
        "        return self._data[key]\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self._data[key] = value\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._data)\n",
        "\n",
        "    def __add__(self, other: \"RNNHidden\") -> \"RNNHidden\":\n",
        "        if other is None:\n",
        "            return self\n",
        "        res = RNNHidden(self._rnn_num + other._rnn_num,\n",
        "                       self._rnn_types + other._rnn_types, self._device, self._batch_first)\n",
        "        res._data = self._data + other._data\n",
        "        return res\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return self._device\n",
        "\n",
        "    def detach(self) -> \"RNNHidden\":\n",
        "        \"\"\"Detach hidden states from computation graph.\"\"\"\n",
        "        result = RNNHidden(self._rnn_num, self._rnn_types, self._device, self._batch_first)\n",
        "        for data, rnn_type in zip(self._data, self._rnn_types):\n",
        "            if isinstance(data, tuple):  # LSTM\n",
        "                result._data.append((data[0].detach(), data[1].detach()))\n",
        "            else:  # GRU and others\n",
        "                result._data.append(data.detach())\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def init_hidden_by_type(rnn_type: str, batch_size: int, unit_num: int,\n",
        "                            device: torch.device) -> Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
        "        \"\"\"Initialize hidden state by RNN type. LSTM returns (h, c) tuple.\"\"\"\n",
        "        if rnn_type == 'lstm':\n",
        "            # LSTM hidden state: (h_0, c_0), each of shape (num_layers, batch, hidden_size)\n",
        "            return (torch.zeros((1, batch_size, unit_num), device=device),\n",
        "                    torch.zeros((1, batch_size, unit_num), device=device))\n",
        "        else:\n",
        "            # GRU and others: single tensor\n",
        "            return torch.zeros((1, batch_size, unit_num), device=device)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 3: RNNBase (From Repository - with LSTM layer support)\n",
        "# ==============================================================================\n",
        "\n",
        "class RNNBase(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for building networks with FC and RNN layers.\n",
        "\n",
        "    Supports layer_type='lstm' or 'gru' for recurrent layers.\n",
        "    This follows the repository's RNNBase pattern.\n",
        "    \"\"\"\n",
        "\n",
        "    ACTIVATION_DICT = {\n",
        "        'tanh': nn.Tanh, 'relu': nn.ReLU, 'sigmoid': nn.Sigmoid,\n",
        "        'leaky_relu': nn.LeakyReLU, 'linear': nn.Identity, 'elu': nn.ELU, 'gelu': nn.GELU,\n",
        "    }\n",
        "\n",
        "    LAYER_DICT = {\n",
        "        'fc': nn.Linear,\n",
        "        'lstm': nn.LSTM,  # LSTM outputs (output, (h_n, c_n))\n",
        "        'gru': nn.GRU,    # GRU outputs (output, h_n)\n",
        "    }\n",
        "\n",
        "    def __init__(self, input_size: int, output_size: int, hidden_sizes: List[int],\n",
        "                 activations: List[str], layer_types: List[str]):\n",
        "        super().__init__()\n",
        "        assert len(activations) == len(hidden_sizes) + 1\n",
        "        assert len(activations) == len(layer_types)\n",
        "\n",
        "        self.layer_types = layer_types\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.activations = nn.ModuleList()\n",
        "        self.rnn_hidden_sizes = []\n",
        "        self.rnn_layer_types = []\n",
        "        self.rnn_num = 0\n",
        "        self.input_size = input_size\n",
        "\n",
        "        sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i, (in_s, out_s) in enumerate(zip(sizes[:-1], sizes[1:])):\n",
        "            ltype = layer_types[i]\n",
        "            if ltype == 'fc':\n",
        "                self.layers.append(nn.Linear(in_s, out_s))\n",
        "            elif ltype in ['gru', 'lstm']:\n",
        "                self.layers.append(self.LAYER_DICT[ltype](in_s, out_s, batch_first=True))\n",
        "                self.rnn_hidden_sizes.append(out_s)\n",
        "                self.rnn_layer_types.append(ltype)\n",
        "                self.rnn_num += 1\n",
        "            self.activations.append(self.ACTIVATION_DICT[activations[i]]())\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                if layer.bias is not None:\n",
        "                    nn.init.constant_(layer.bias, 0)\n",
        "            elif isinstance(layer, (nn.LSTM, nn.GRU)):\n",
        "                for name, param in layer.named_parameters():\n",
        "                    if 'weight_ih' in name:\n",
        "                        nn.init.xavier_uniform_(param)\n",
        "                    elif 'weight_hh' in name:\n",
        "                        nn.init.orthogonal_(param)\n",
        "                    elif 'bias' in name:\n",
        "                        nn.init.zeros_(param)\n",
        "\n",
        "    def make_init_state(self, batch_size: int, device: torch.device) -> RNNHidden:\n",
        "        \"\"\"Create initial hidden state for all RNN layers.\"\"\"\n",
        "        hidden = RNNHidden(self.rnn_num, self.rnn_layer_types, device)\n",
        "        for size, rnn_type in zip(self.rnn_hidden_sizes, self.rnn_layer_types):\n",
        "            hidden.append(RNNHidden.init_hidden_by_type(rnn_type, batch_size, size, device))\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, x: torch.Tensor, hidden: Optional[RNNHidden] = None,\n",
        "                return_full: bool = False) -> Tuple[torch.Tensor, RNNHidden, Optional[RNNHidden]]:\n",
        "        if hidden is None:\n",
        "            hidden = self.make_init_state(x.shape[0], x.device)\n",
        "\n",
        "        x_dim = len(x.shape)\n",
        "        if x_dim == 2 and self.rnn_num > 0:\n",
        "            x = x.unsqueeze(1)  # Add sequence dimension\n",
        "\n",
        "        out_hidden = RNNHidden(self.rnn_num, self.rnn_layer_types, x.device)\n",
        "        full_hidden = RNNHidden(self.rnn_num, self.rnn_layer_types, x.device, batch_first=True) if return_full else None\n",
        "\n",
        "        rnn_idx = 0\n",
        "        for layer, activation, ltype in zip(self.layers, self.activations, self.layer_types):\n",
        "            if ltype in ['gru', 'lstm']:\n",
        "                h = hidden[rnn_idx]\n",
        "                if ltype == 'lstm':\n",
        "                    # LSTM: hidden is (h, c) tuple\n",
        "                    x, (h_n, c_n) = layer(x, h)\n",
        "                    out_hidden.append((h_n, c_n))\n",
        "                else:\n",
        "                    # GRU: hidden is single tensor\n",
        "                    x, h_n = layer(x, h)\n",
        "                    out_hidden.append(h_n)\n",
        "                if return_full:\n",
        "                    full_hidden.append(x)\n",
        "                rnn_idx += 1\n",
        "            else:\n",
        "                x = layer(x)\n",
        "            x = activation(x)\n",
        "\n",
        "        if x_dim == 2 and self.rnn_num > 0:\n",
        "            x = x.squeeze(1)\n",
        "\n",
        "        return x, out_hidden, full_hidden\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 4: Contextual Model (Embedding + Universal Network) - From Repository\n",
        "# ==============================================================================\n",
        "\n",
        "class ContextualModel:\n",
        "    \"\"\"Two-stage model: embedding network processes history, universal network outputs action/value.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_in: int, embed_out: int, embed_hidden: List[int],\n",
        "                 embed_acts: List[str], embed_types: List[str],\n",
        "                 uni_in: int, uni_out: int, uni_hidden: List[int],\n",
        "                 uni_acts: List[str], uni_types: List[str], name: str = 'ContextualModel'):\n",
        "        self.name = name\n",
        "        self.embedding_net = RNNBase(embed_in, embed_out, embed_hidden, embed_acts, embed_types)\n",
        "        self.uni_net = RNNBase(embed_out + uni_in, uni_out, uni_hidden, uni_acts, uni_types)\n",
        "        self.rnn_num = self.embedding_net.rnn_num + self.uni_net.rnn_num\n",
        "        self.device = torch.device('cpu')\n",
        "        self._modules = {'embedding': self.embedding_net, 'universal': self.uni_net}\n",
        "\n",
        "    def parameters(self, recurse: bool = True) -> List[torch.Tensor]:\n",
        "        return list(self.embedding_net.parameters(recurse)) + list(self.uni_net.parameters(recurse))\n",
        "\n",
        "    def to(self, device: torch.device):\n",
        "        if device != self.device:\n",
        "            self.device = device\n",
        "            self.embedding_net.to(device)\n",
        "            self.uni_net.to(device)\n",
        "        return self\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        self.embedding_net.train(mode)\n",
        "        self.uni_net.train(mode)\n",
        "\n",
        "    def eval(self):\n",
        "        self.train(False)\n",
        "\n",
        "    def make_init_state(self, batch_size: int, device: torch.device) -> RNNHidden:\n",
        "        return self.embedding_net.make_init_state(batch_size, device) + \\\n",
        "               self.uni_net.make_init_state(batch_size, device)\n",
        "\n",
        "    def meta_forward(self, embed_input: torch.Tensor, uni_input: torch.Tensor,\n",
        "                     hidden: Optional[RNNHidden] = None, detach_embed: bool = False\n",
        "                     ) -> Tuple[torch.Tensor, RNNHidden, torch.Tensor]:\n",
        "        if hidden is None:\n",
        "            hidden = self.make_init_state(embed_input.shape[0], embed_input.device)\n",
        "\n",
        "        embed_hidden = hidden[:self.embedding_net.rnn_num] if len(hidden) > 0 else None\n",
        "        uni_hidden = hidden[self.embedding_net.rnn_num:] if len(hidden) > 0 else None\n",
        "\n",
        "        embed_out, embed_h, _ = self.embedding_net.forward(embed_input, embed_hidden, False)\n",
        "\n",
        "        if detach_embed:\n",
        "            embed_out = embed_out.detach()\n",
        "\n",
        "        if len(embed_out.shape) - len(uni_input.shape) == 1:\n",
        "            uni_input = uni_input.unsqueeze(1).expand(embed_out.shape[0], embed_out.shape[1], -1)\n",
        "\n",
        "        combined = torch.cat([uni_input, embed_out], dim=-1)\n",
        "        uni_out, uni_h, _ = self.uni_net.forward(combined, uni_hidden, False)\n",
        "\n",
        "        return uni_out, embed_h + uni_h, embed_out\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {k: v.state_dict() for k, v in self._modules.items()}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        for k, v in self._modules.items():\n",
        "            v.load_state_dict(state_dict[k])\n",
        "\n",
        "    def save(self, path: str):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        for k, v in self._modules.items():\n",
        "            torch.save(v.state_dict(), os.path.join(path, f'{self.name}-{k}.pt'))\n",
        "\n",
        "    def load(self, path: str, map_location=None):\n",
        "        for k, v in self._modules.items():\n",
        "            v.load_state_dict(torch.load(os.path.join(path, f'{self.name}-{k}.pt'),\n",
        "                                         map_location=map_location))\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \"\"\"Make ContextualModel callable like nn.Module.\"\"\"\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 5: LSTM-SAC Policy Network (Actor) - Using Repository Pattern with LSTM\n",
        "# ==============================================================================\n",
        "\n",
        "class RecurrentSACPolicy(ContextualModel):\n",
        "    \"\"\"\n",
        "    Recurrent SAC policy (actor) with LSTM-based context encoding.\n",
        "\n",
        "    Uses layer_type='lstm' instead of 'gru' for LSTM recurrence.\n",
        "    \"\"\"\n",
        "\n",
        "    MAX_LOG_STD = 2.0\n",
        "    MIN_LOG_STD = -20.0\n",
        "\n",
        "    def __init__(self, state_dim: int, action_dim: int, embed_dim: int = 64,\n",
        "                 embed_hidden: List[int] = [64], uni_hidden: List[int] = [256, 256],\n",
        "                 use_last_action: bool = True):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.use_last_action = use_last_action\n",
        "\n",
        "        embed_in = state_dim\n",
        "        if use_last_action:\n",
        "            embed_in += action_dim\n",
        "\n",
        "        # Use LSTM instead of GRU\n",
        "        embed_acts = ['relu'] * len(embed_hidden) + ['relu']\n",
        "        embed_types = ['lstm'] * (len(embed_hidden) + 1)  # LSTM layers\n",
        "        uni_acts = ['relu'] * len(uni_hidden) + ['linear']\n",
        "        uni_types = ['fc'] * (len(uni_hidden) + 1)\n",
        "\n",
        "        super().__init__(embed_in, embed_dim, embed_hidden, embed_acts, embed_types,\n",
        "                        state_dim, action_dim * 2, uni_hidden, uni_acts, uni_types,\n",
        "                        name='RecurrentSACPolicy')\n",
        "\n",
        "    def get_embed_input(self, state: torch.Tensor, last_action: torch.Tensor) -> torch.Tensor:\n",
        "        inputs = [state]\n",
        "        if self.use_last_action:\n",
        "            inputs.append(last_action)\n",
        "        return torch.cat(inputs, dim=-1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, last_action: torch.Tensor,\n",
        "                hidden: Optional[RNNHidden] = None\n",
        "                ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, RNNHidden]:\n",
        "        embed_input = self.get_embed_input(state, last_action)\n",
        "        output, hidden, embed_out = self.meta_forward(embed_input, state, hidden)\n",
        "\n",
        "        mean, log_std = output.chunk(2, dim=-1)\n",
        "        log_std = torch.clamp(log_std, self.MIN_LOG_STD, self.MAX_LOG_STD)\n",
        "        std = log_std.exp()\n",
        "\n",
        "        noise = torch.randn_like(mean)\n",
        "        sample = mean + noise * std\n",
        "\n",
        "        log_prob = (-0.5 * noise.pow(2) - log_std - 0.5 * np.log(2 * np.pi)).sum(-1, keepdim=True)\n",
        "        log_prob -= (2 * (np.log(2) - sample - F.softplus(-2 * sample))).sum(-1, keepdim=True)\n",
        "\n",
        "        action_mean = torch.tanh(mean)\n",
        "        action_sample = torch.tanh(sample)\n",
        "\n",
        "        return action_mean, action_sample, log_prob, hidden\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 6: LSTM-SAC Value Network (Critic) - Using Repository Pattern with LSTM\n",
        "# ==============================================================================\n",
        "\n",
        "class RecurrentSACValue(ContextualModel):\n",
        "    \"\"\"\n",
        "    Recurrent SAC Q-value network (critic) with LSTM-based context encoding.\n",
        "\n",
        "    Uses layer_type='lstm' instead of 'gru' for LSTM recurrence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dim: int, action_dim: int, embed_dim: int = 64,\n",
        "                 embed_hidden: List[int] = [64], uni_hidden: List[int] = [256, 256],\n",
        "                 use_last_action: bool = True):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.use_last_action = use_last_action\n",
        "\n",
        "        embed_in = state_dim\n",
        "        if use_last_action:\n",
        "            embed_in += action_dim\n",
        "\n",
        "        # Use LSTM instead of GRU\n",
        "        embed_acts = ['relu'] * len(embed_hidden) + ['relu']\n",
        "        embed_types = ['lstm'] * (len(embed_hidden) + 1)  # LSTM layers\n",
        "        uni_acts = ['relu'] * len(uni_hidden) + ['linear']\n",
        "        uni_types = ['fc'] * (len(uni_hidden) + 1)\n",
        "\n",
        "        super().__init__(embed_in, embed_dim, embed_hidden, embed_acts, embed_types,\n",
        "                        state_dim + action_dim, 1, uni_hidden, uni_acts, uni_types,\n",
        "                        name='RecurrentSACValue')\n",
        "\n",
        "    def get_embed_input(self, state: torch.Tensor, last_action: torch.Tensor) -> torch.Tensor:\n",
        "        inputs = [state]\n",
        "        if self.use_last_action:\n",
        "            inputs.append(last_action)\n",
        "        return torch.cat(inputs, dim=-1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, last_action: torch.Tensor, action: torch.Tensor,\n",
        "                hidden: Optional[RNNHidden] = None, detach_embed: bool = False\n",
        "                ) -> Tuple[torch.Tensor, RNNHidden]:\n",
        "        embed_input = self.get_embed_input(state, last_action)\n",
        "        uni_input = torch.cat([state, action], dim=-1)\n",
        "        q_value, hidden, embed_out = self.meta_forward(embed_input, uni_input, hidden, detach_embed)\n",
        "        return q_value, hidden\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 7: Trajectory Replay Buffer\n",
        "# ==============================================================================\n",
        "\n",
        "Transition = namedtuple('Transition', [\n",
        "    'state', 'last_action', 'action', 'next_state', 'reward', 'done', 'timeout'\n",
        "])\n",
        "\n",
        "\n",
        "class SequenceReplayBuffer:\n",
        "    \"\"\"Replay buffer storing full trajectories for sequence training.\"\"\"\n",
        "\n",
        "    def __init__(self, max_trajectories: int = 1000, max_length: int = 1000):\n",
        "        self.max_trajectories = max_trajectories\n",
        "        self.max_length = max_length\n",
        "        self.buffer: Optional[np.ndarray] = None\n",
        "        self.lengths = [0] * max_trajectories\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "        self.total_steps = 0\n",
        "        self.current_trajectory: List[Transition] = []\n",
        "        self.dim_info: Optional[List[Tuple[int, int]]] = None\n",
        "\n",
        "    def _init_buffer(self, transition: Transition):\n",
        "        self.dim_info = []\n",
        "        total_dim = 0\n",
        "        for item in transition:\n",
        "            dim = item.shape[-1] if isinstance(item, np.ndarray) else 1\n",
        "            self.dim_info.append((total_dim, total_dim + dim))\n",
        "            total_dim += dim\n",
        "        self.buffer = np.zeros((self.max_trajectories, self.max_length, total_dim))\n",
        "\n",
        "    def _transition_to_array(self, t: Transition) -> np.ndarray:\n",
        "        arrays = []\n",
        "        for item in t:\n",
        "            if isinstance(item, np.ndarray):\n",
        "                arrays.append(item.flatten())\n",
        "            else:\n",
        "                arrays.append(np.array([item]))\n",
        "        return np.concatenate(arrays)\n",
        "\n",
        "    def _array_to_dict(self, data: np.ndarray) -> Dict[str, np.ndarray]:\n",
        "        fields = Transition._fields\n",
        "        result = {}\n",
        "        for i, field in enumerate(fields):\n",
        "            start, end = self.dim_info[i]\n",
        "            result[field] = data[..., start:end]\n",
        "        return result\n",
        "\n",
        "    def push(self, transition: Transition):\n",
        "        self.current_trajectory.append(transition)\n",
        "        if transition.done:\n",
        "            self._complete_trajectory()\n",
        "\n",
        "    def _complete_trajectory(self):\n",
        "        if len(self.current_trajectory) == 0:\n",
        "            return\n",
        "        if self.buffer is None:\n",
        "            self._init_buffer(self.current_trajectory[0])\n",
        "\n",
        "        traj_len = min(len(self.current_trajectory), self.max_length)\n",
        "        self.buffer[self.ptr] = 0\n",
        "\n",
        "        for i in range(traj_len):\n",
        "            self.buffer[self.ptr, i] = self._transition_to_array(self.current_trajectory[i])\n",
        "\n",
        "        self.total_steps -= self.lengths[self.ptr]\n",
        "        self.lengths[self.ptr] = traj_len\n",
        "        self.total_steps += traj_len\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.max_trajectories\n",
        "        self.size = min(self.size + 1, self.max_trajectories)\n",
        "        self.current_trajectory = []\n",
        "\n",
        "    def sample_chunks(self, batch_size: int, chunk_len: int = 64) -> Tuple[Dict[str, np.ndarray], np.ndarray, int]:\n",
        "        if self.size == 0:\n",
        "            raise ValueError(\"Buffer is empty\")\n",
        "\n",
        "        chunks = []\n",
        "        masks = []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            traj_idx = np.random.randint(0, self.size)\n",
        "            traj_len = self.lengths[traj_idx]\n",
        "\n",
        "            if traj_len <= chunk_len:\n",
        "                chunk = self.buffer[traj_idx, :traj_len]\n",
        "                pad_len = chunk_len - traj_len\n",
        "                if pad_len > 0:\n",
        "                    pad = np.zeros((pad_len, chunk.shape[-1]))\n",
        "                    chunk = np.vstack([chunk, pad])\n",
        "                mask = np.zeros((chunk_len, 1))\n",
        "                mask[:traj_len] = 1\n",
        "            else:\n",
        "                start = np.random.randint(0, traj_len - chunk_len + 1)\n",
        "                chunk = self.buffer[traj_idx, start:start + chunk_len]\n",
        "                mask = np.ones((chunk_len, 1))\n",
        "\n",
        "            chunks.append(chunk)\n",
        "            masks.append(mask)\n",
        "\n",
        "        chunks = np.stack(chunks, axis=0)\n",
        "        masks = np.stack(masks, axis=0)\n",
        "        data = self._array_to_dict(chunks)\n",
        "        return data, masks, int(masks.sum())\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 8: Utility Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def normalize_action(action: np.ndarray, action_space) -> np.ndarray:\n",
        "    return (action - action_space.low) / (action_space.high - action_space.low) * 2 - 1\n",
        "\n",
        "def denormalize_action(action: np.ndarray, action_space) -> np.ndarray:\n",
        "    return (action + 1) / 2 * (action_space.high - action_space.low) + action_space.low\n",
        "\n",
        "def to_torch(data: np.ndarray, device: torch.device) -> torch.Tensor:\n",
        "    return torch.from_numpy(data).float().to(device)\n",
        "\n",
        "def to_numpy(tensor: torch.Tensor) -> np.ndarray:\n",
        "    return tensor.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 9: LSTM-SAC Agent\n",
        "# ==============================================================================\n",
        "\n",
        "class LSTMSAC:\n",
        "    \"\"\"\n",
        "    LSTM-based Soft Actor-Critic Agent.\n",
        "\n",
        "    Uses the repository's ContextualModel pattern with LSTM layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, seed: int = 0, gamma: float = 0.99, tau: float = 0.002,\n",
        "                 alpha: float = 0.2, lr: float = 1e-4, buffer_size: int = 1000,\n",
        "                 batch_size: int = 128, embed_dim: int = 64,\n",
        "                 embed_hidden: List[int] = [64], uni_hidden: List[int] = [256, 256],\n",
        "                 auto_alpha: bool = True, warmup_steps: int = 1000,\n",
        "                 update_interval: int = 4, max_traj_len: int = 600,\n",
        "                 chunk_len: int = 96, burn_in: int = 32, save_best_path: str = './models/best'):\n",
        "\n",
        "        self._set_seed(seed)\n",
        "        self.seed = seed\n",
        "\n",
        "        self.env = env\n",
        "        self.eval_env = create_AhnChemoEnv_setting1()\n",
        "\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        self.max_traj_len = min(max_traj_len, getattr(env, 'max_t', max_traj_len))\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        print(f\"LSTM config: embed_dim={embed_dim}, embed_hidden={embed_hidden}\")\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.update_interval = update_interval\n",
        "        self.auto_alpha = auto_alpha\n",
        "        self.chunk_len = chunk_len\n",
        "        self.burn_in = burn_in\n",
        "\n",
        "        # Networks using repository pattern with LSTM\n",
        "        self.policy = RecurrentSACPolicy(\n",
        "            self.state_dim, self.action_dim, embed_dim, embed_hidden, uni_hidden\n",
        "        )\n",
        "        self.policy.to(self.device)\n",
        "\n",
        "        self.q1 = RecurrentSACValue(\n",
        "            self.state_dim, self.action_dim, embed_dim, embed_hidden, uni_hidden\n",
        "        )\n",
        "        self.q1.to(self.device)\n",
        "\n",
        "        self.q2 = RecurrentSACValue(\n",
        "            self.state_dim, self.action_dim, embed_dim, embed_hidden, uni_hidden\n",
        "        )\n",
        "        self.q2.to(self.device)\n",
        "\n",
        "        self.q1_target = RecurrentSACValue(\n",
        "            self.state_dim, self.action_dim, embed_dim, embed_hidden, uni_hidden\n",
        "        )\n",
        "        self.q1_target.to(self.device)\n",
        "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
        "\n",
        "        self.q2_target = RecurrentSACValue(\n",
        "            self.state_dim, self.action_dim, embed_dim, embed_hidden, uni_hidden\n",
        "        )\n",
        "        self.q2_target.to(self.device)\n",
        "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
        "\n",
        "        # Entropy\n",
        "        self.target_entropy = -self.action_dim\n",
        "        self.log_alpha = torch.tensor(np.log(alpha), device=self.device, requires_grad=True)\n",
        "\n",
        "        # Optimizers\n",
        "        self.policy_optim = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.q1_optim = torch.optim.Adam(self.q1.parameters(), lr=lr)\n",
        "        self.q2_optim = torch.optim.Adam(self.q2.parameters(), lr=lr)\n",
        "        self.alpha_optim = torch.optim.Adam([self.log_alpha], lr=lr)\n",
        "\n",
        "        # Replay buffer\n",
        "        self.buffer = SequenceReplayBuffer(buffer_size, self.max_traj_len)\n",
        "\n",
        "        # Tracking\n",
        "        self.total_steps = 0\n",
        "        self.episodes = 0\n",
        "        self.best_eval_reward = -float('inf')\n",
        "        self.save_best_path = save_best_path\n",
        "        self.reward_history: List[Dict] = []\n",
        "\n",
        "    def _set_seed(self, seed: int):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    def _reset_env(self, env, seed=None):\n",
        "        try:\n",
        "            if seed is not None:\n",
        "                obs, _ = env.reset(seed=seed)\n",
        "            else:\n",
        "                obs, _ = env.reset()\n",
        "        except TypeError:\n",
        "            result = env.reset()\n",
        "            obs = result[0] if isinstance(result, tuple) else result\n",
        "        return obs\n",
        "\n",
        "    def _step_env(self, env, action):\n",
        "        result = env.step(action)\n",
        "        if len(result) == 5:\n",
        "            obs, reward, terminated, truncated, info = result\n",
        "            done = terminated or truncated\n",
        "        else:\n",
        "            obs, reward, done, info = result\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    @property\n",
        "    def alpha(self) -> torch.Tensor:\n",
        "        return self.log_alpha.exp()\n",
        "\n",
        "    def select_action(self, state: np.ndarray, last_action: np.ndarray,\n",
        "                      hidden: RNNHidden, deterministic: bool = False\n",
        "                      ) -> Tuple[np.ndarray, RNNHidden]:\n",
        "        with torch.no_grad():\n",
        "            state_t = to_torch(state.reshape(1, -1), self.device)\n",
        "            last_action_t = to_torch(last_action.reshape(1, -1), self.device)\n",
        "\n",
        "            action_mean, action_sample, _, new_hidden = self.policy(\n",
        "                state_t, last_action_t, hidden\n",
        "            )\n",
        "\n",
        "            action = action_mean if deterministic else action_sample\n",
        "            return to_numpy(action).squeeze(0), new_hidden\n",
        "\n",
        "    def _masked_mean(self, data: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "        return (data * mask).sum() / mask.sum().clamp(min=1)\n",
        "\n",
        "    def _soft_update(self, target: ContextualModel, source: ContextualModel):\n",
        "        with torch.no_grad():\n",
        "            for tp, sp in zip(target.parameters(), source.parameters()):\n",
        "                tp.data.copy_(self.tau * sp.data + (1 - self.tau) * tp.data)\n",
        "\n",
        "    def update(self) -> Dict[str, float]:\n",
        "        train_len = self.chunk_len - self.burn_in\n",
        "        if train_len <= 0:\n",
        "            raise ValueError(f\"chunk_len ({self.chunk_len}) must be > burn_in ({self.burn_in})\")\n",
        "\n",
        "        data, mask, _ = self.buffer.sample_chunks(self.batch_size, chunk_len=self.chunk_len)\n",
        "\n",
        "        state = to_torch(data['state'], self.device)\n",
        "        last_action = to_torch(data['last_action'], self.device)\n",
        "        action = to_torch(data['action'], self.device)\n",
        "        next_state = to_torch(data['next_state'], self.device)\n",
        "        reward = to_torch(data['reward'], self.device)\n",
        "        done = to_torch(data['done'], self.device)\n",
        "        timeout = to_torch(data['timeout'], self.device)\n",
        "        mask_t = to_torch(mask.copy(), self.device)\n",
        "\n",
        "        # Burn-in phase\n",
        "        with torch.no_grad():\n",
        "            state_burn = state[:, :self.burn_in]\n",
        "            last_action_burn = last_action[:, :self.burn_in]\n",
        "            n_trajs = state.shape[0]\n",
        "\n",
        "            policy_h = self.policy.make_init_state(n_trajs, self.device)\n",
        "            q1_h = self.q1.make_init_state(n_trajs, self.device)\n",
        "            q2_h = self.q2.make_init_state(n_trajs, self.device)\n",
        "            q1t_h = self.q1_target.make_init_state(n_trajs, self.device)\n",
        "            q2t_h = self.q2_target.make_init_state(n_trajs, self.device)\n",
        "\n",
        "            if self.burn_in > 0:\n",
        "                _, _, _, policy_h = self.policy(state_burn, last_action_burn, policy_h)\n",
        "                _, q1_h = self.q1(state_burn, last_action_burn, action[:, :self.burn_in], q1_h)\n",
        "                _, q2_h = self.q2(state_burn, last_action_burn, action[:, :self.burn_in], q2_h)\n",
        "                _, q1t_h = self.q1_target(state_burn, last_action_burn, action[:, :self.burn_in], q1t_h)\n",
        "                _, q2t_h = self.q2_target(state_burn, last_action_burn, action[:, :self.burn_in], q2t_h)\n",
        "\n",
        "        # Training phase\n",
        "        state_train = state[:, self.burn_in:]\n",
        "        last_action_train = last_action[:, self.burn_in:]\n",
        "        action_train = action[:, self.burn_in:]\n",
        "        next_state_train = next_state[:, self.burn_in:]\n",
        "        reward_train = reward[:, self.burn_in:]\n",
        "        done_train = done[:, self.burn_in:]\n",
        "        timeout_train = timeout[:, self.burn_in:]\n",
        "        mask_train = mask_t[:, self.burn_in:]\n",
        "\n",
        "        done_train = done_train * (1 - timeout_train)\n",
        "        alpha = self.alpha.detach()\n",
        "\n",
        "        # Target Q\n",
        "        with torch.no_grad():\n",
        "            _, next_action, next_log_prob, _ = self.policy(next_state_train, action_train, policy_h.detach())\n",
        "            q1_next, _ = self.q1_target(next_state_train, action_train, next_action, q1t_h)\n",
        "            q2_next, _ = self.q2_target(next_state_train, action_train, next_action, q2t_h)\n",
        "            q_next = torch.min(q1_next, q2_next) - alpha * next_log_prob\n",
        "            target_q = reward_train + (1 - done_train) * self.gamma * q_next\n",
        "\n",
        "        # Critic update\n",
        "        q1_pred, _ = self.q1(state_train, last_action_train, action_train, q1_h)\n",
        "        q2_pred, _ = self.q2(state_train, last_action_train, action_train, q2_h)\n",
        "\n",
        "        q1_loss = self._masked_mean((q1_pred - target_q).pow(2), mask_train)\n",
        "        q2_loss = self._masked_mean((q2_pred - target_q).pow(2), mask_train)\n",
        "\n",
        "        self.q1_optim.zero_grad()\n",
        "        q1_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q1.parameters(), max_norm=10.0)\n",
        "        self.q1_optim.step()\n",
        "\n",
        "        self.q2_optim.zero_grad()\n",
        "        q2_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q2.parameters(), max_norm=10.0)\n",
        "        self.q2_optim.step()\n",
        "\n",
        "        # Actor update\n",
        "        _, new_action, log_prob, _ = self.policy(state_train, last_action_train, policy_h.detach())\n",
        "        q1_pi, _ = self.q1(state_train, last_action_train, new_action, q1_h.detach(), detach_embed=True)\n",
        "        q2_pi, _ = self.q2(state_train, last_action_train, new_action, q2_h.detach(), detach_embed=True)\n",
        "        q_pi = torch.min(q1_pi, q2_pi)\n",
        "\n",
        "        policy_loss = self._masked_mean(alpha * log_prob - q_pi, mask_train)\n",
        "\n",
        "        self.policy_optim.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=10.0)\n",
        "        self.policy_optim.step()\n",
        "\n",
        "        # Alpha update\n",
        "        alpha_loss = torch.tensor(0.0)\n",
        "        if self.auto_alpha:\n",
        "            alpha_loss = -self._masked_mean(\n",
        "                self.log_alpha * (log_prob + self.target_entropy).detach(), mask_train\n",
        "            )\n",
        "            self.alpha_optim.zero_grad()\n",
        "            alpha_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_([self.log_alpha], max_norm=10.0)\n",
        "            self.alpha_optim.step()\n",
        "\n",
        "            # Clamp alpha to prevent collapse\n",
        "            with torch.no_grad():\n",
        "                self.log_alpha.clamp_(min=np.log(0.12))\n",
        "\n",
        "        # Soft update targets\n",
        "        self._soft_update(self.q1_target, self.q1)\n",
        "        self._soft_update(self.q2_target, self.q2)\n",
        "\n",
        "        return {\n",
        "            'q1_loss': q1_loss.item(),\n",
        "            'q2_loss': q2_loss.item(),\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'alpha': self.alpha.item(),\n",
        "        }\n",
        "\n",
        "    def train(self, total_steps: int = 50000, eval_interval: int = 5000,\n",
        "              log_interval: int = 1000) -> Tuple[List[float], List[int]]:\n",
        "        print(f\"Starting LSTM-SAC training for {total_steps} steps...\")\n",
        "\n",
        "        ep_rewards = []\n",
        "        ep_lengths = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        obs = self._reset_env(self.env, self.seed)\n",
        "        last_action = np.zeros(self.action_dim)\n",
        "        hidden = self.policy.make_init_state(1, self.device)\n",
        "        ep_reward = 0.0\n",
        "        ep_length = 0\n",
        "        losses = {'alpha': self.alpha.item()}\n",
        "\n",
        "        for step in range(1, total_steps + 1):\n",
        "            self.total_steps = step\n",
        "\n",
        "            if step < self.warmup_steps:\n",
        "                action = self.env.action_space.sample()\n",
        "            else:\n",
        "                action, hidden = self.select_action(obs, last_action, hidden)\n",
        "\n",
        "            env_action = denormalize_action(action, self.env.action_space)\n",
        "            next_obs, reward, done, info = self._step_env(self.env, env_action)\n",
        "            timeout = info.get('TimeLimit.truncated', False) if isinstance(info, dict) else False\n",
        "\n",
        "            transition = Transition(\n",
        "                state=obs.copy(),\n",
        "                last_action=last_action.copy(),\n",
        "                action=action.copy() if isinstance(action, np.ndarray) else np.array([action]),\n",
        "                next_state=next_obs.copy(),\n",
        "                reward=np.array([reward]),\n",
        "                done=np.array([float(done)]),\n",
        "                timeout=np.array([float(timeout)])\n",
        "            )\n",
        "            self.buffer.push(transition)\n",
        "\n",
        "            ep_reward += reward\n",
        "            ep_length += 1\n",
        "            last_action = action.copy() if isinstance(action, np.ndarray) else np.array([action])\n",
        "            obs = next_obs\n",
        "\n",
        "            if done:\n",
        "                ep_rewards.append(ep_reward)\n",
        "                ep_lengths.append(ep_length)\n",
        "                self.episodes += 1\n",
        "\n",
        "                cumulative_steps = sum(ep_lengths)\n",
        "                self.reward_history.append({\n",
        "                    'episode': self.episodes,\n",
        "                    'timestep': cumulative_steps,\n",
        "                    'reward': ep_reward,\n",
        "                    'length': ep_length\n",
        "                })\n",
        "\n",
        "                obs = self._reset_env(self.env)\n",
        "                last_action = np.zeros(self.action_dim)\n",
        "                hidden = self.policy.make_init_state(1, self.device)\n",
        "                ep_reward = 0.0\n",
        "                ep_length = 0\n",
        "\n",
        "            if step >= self.warmup_steps and len(self.buffer) > 0:\n",
        "                if step % self.update_interval == 0:\n",
        "                    losses = self.update()\n",
        "\n",
        "                if step % log_interval == 0:\n",
        "                    avg_reward = np.mean(ep_rewards[-10:]) if ep_rewards else 0\n",
        "                    elapsed = time.time() - start_time\n",
        "                    print(f\"Step {step}/{total_steps} | Eps: {self.episodes} | \"\n",
        "                          f\"Reward: {avg_reward:.1f} | Alpha: {losses['alpha']:.4f} | \"\n",
        "                          f\"Time: {elapsed:.0f}s\")\n",
        "\n",
        "                    csv_path = os.path.join(\"./results\", \"reward.csv\")\n",
        "                    self.save_rewards_csv(csv_path)\n",
        "\n",
        "            if step % eval_interval == 0:\n",
        "                eval_reward = self.evaluate(n_episodes=5)\n",
        "                print(f\"[EVAL] Step {step} | Mean Reward: {eval_reward:.2f}\")\n",
        "\n",
        "                # Save best model checkpoint\n",
        "                if eval_reward > self.best_eval_reward:\n",
        "                    self.best_eval_reward = eval_reward\n",
        "                    self.save(self.save_best_path)\n",
        "                    print(f\"[BEST] New best model saved! Reward: {eval_reward:.2f}\")\n",
        "\n",
        "        print(f\"\\nTraining completed in {time.time() - start_time:.1f}s\")\n",
        "        return ep_rewards, ep_lengths\n",
        "\n",
        "    def evaluate(self, n_episodes: int = 10) -> float:\n",
        "        self.policy.eval()\n",
        "        total_reward = 0.0\n",
        "\n",
        "        for _ in range(n_episodes):\n",
        "            obs = self._reset_env(self.eval_env)\n",
        "            last_action = np.zeros(self.action_dim)\n",
        "            hidden = self.policy.make_init_state(1, self.device)\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action, hidden = self.select_action(obs, last_action, hidden, deterministic=True)\n",
        "                env_action = denormalize_action(action, self.eval_env.action_space)\n",
        "                obs, reward, done, _ = self._step_env(self.eval_env, env_action)\n",
        "                total_reward += reward\n",
        "                last_action = action.copy() if isinstance(action, np.ndarray) else np.array([action])\n",
        "\n",
        "        self.policy.train()\n",
        "        return total_reward / n_episodes\n",
        "\n",
        "    def save(self, path: str):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        self.policy.save(path)\n",
        "        self.q1.save(path)\n",
        "        self.q2.save(path)\n",
        "        torch.save({'log_alpha': self.log_alpha}, os.path.join(path, 'alpha.pt'))\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load(self, path: str):\n",
        "        self.policy.load(path, map_location=self.device)\n",
        "        self.q1.load(path, map_location=self.device)\n",
        "        self.q2.load(path, map_location=self.device)\n",
        "        checkpoint = torch.load(os.path.join(path, 'alpha.pt'), map_location=self.device)\n",
        "        self.log_alpha = checkpoint['log_alpha']\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "    def save_rewards_csv(self, path: str):\n",
        "        if len(self.reward_history) == 0:\n",
        "            return\n",
        "        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)\n",
        "        df = pd.DataFrame(self.reward_history)\n",
        "        df.to_csv(path, index=False)\n",
        "        print(f\"Rewards saved to: {path}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 10: Plotting Function\n",
        "# ==============================================================================\n",
        "\n",
        "def plot_training_reward_curve(ep_rewards: List[float], experiment_name: str,\n",
        "                               ep_lengths: Optional[List[int]] = None,\n",
        "                               window_size: int = 10, save_path: Optional[str] = None):\n",
        "    \"\"\"Plot training reward curve with rolling statistics.\"\"\"\n",
        "    print(f\"\\n--- Plotting Training Reward Curve ---\")\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'episode': list(range(1, len(ep_rewards) + 1)),\n",
        "        'r': ep_rewards,\n",
        "        'l': ep_lengths if ep_lengths else [600] * len(ep_rewards)\n",
        "    })\n",
        "    results_df['timesteps'] = results_df['l'].cumsum()\n",
        "    results_df['reward_mean'] = results_df['r'].rolling(window=window_size, min_periods=1).mean()\n",
        "    results_df['reward_std'] = results_df['r'].rolling(window=window_size, min_periods=1).std().fillna(0)\n",
        "    results_df['lower_bound'] = results_df['reward_mean'] - results_df['reward_std']\n",
        "    results_df['upper_bound'] = results_df['reward_mean'] + results_df['reward_std']\n",
        "\n",
        "    if HAS_SEABORN:\n",
        "        sns.set(style=\"darkgrid\", font_scale=1.2)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    color = sns.color_palette()[0] if HAS_SEABORN else 'blue'\n",
        "\n",
        "    ax1 = axes[0]\n",
        "    ax1.fill_between(results_df['episode'], results_df['lower_bound'],\n",
        "                     results_df['upper_bound'], alpha=0.2, color=color)\n",
        "    ax1.plot(results_df['episode'], results_df['r'], alpha=0.15, color='gray')\n",
        "    ax1.plot(results_df['episode'], results_df['reward_mean'], linewidth=2, color=color)\n",
        "    ax1.set_title(f'{experiment_name} - Reward Over Episodes')\n",
        "    ax1.set_xlabel('Episode')\n",
        "    ax1.set_ylabel('Reward')\n",
        "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    ax2 = axes[1]\n",
        "    ax2.fill_between(results_df['timesteps'], results_df['lower_bound'],\n",
        "                     results_df['upper_bound'], alpha=0.2, color=color)\n",
        "    ax2.plot(results_df['timesteps'], results_df['r'], alpha=0.15, color='gray')\n",
        "    ax2.plot(results_df['timesteps'], results_df['reward_mean'], linewidth=2, color=color)\n",
        "    ax2.set_title(f'{experiment_name} - Reward Over Timesteps')\n",
        "    ax2.set_xlabel('Timesteps')\n",
        "    ax2.set_ylabel('Reward')\n",
        "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Plot saved to: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Total Episodes: {len(results_df)}\")\n",
        "    print(f\"Total Timesteps: {results_df['timesteps'].iloc[-1]:,}\")\n",
        "    print(f\"Best Reward: {results_df['r'].max():.2f}\")\n",
        "    print(f\"Mean Reward: {results_df['r'].mean():.2f}  {results_df['r'].std():.2f}\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 11: Main Entry Point\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LSTM-SAC for Cancer Chemotherapy Environment\")\n",
        "    print(\"(Using Repository RNNBase/ContextualModel Pattern with LSTM)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    TOTAL_TIMESTEPS = 400000\n",
        "    EXPERIMENT_NAME = \"LSTM_SAC_ENV1\"\n",
        "    RESULTS_DIR = \"./results\"\n",
        "    MODELS_DIR = \"./models\"\n",
        "\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "    env = create_AhnChemoEnv_setting1()\n",
        "\n",
        "    # agent = LSTMSAC(\n",
        "    #     env=env,\n",
        "    #     seed=42,\n",
        "    #     gamma=0.99,\n",
        "    #     tau=0.005,\n",
        "    #     batch_size=128,\n",
        "    #     embed_dim=64,\n",
        "    #     embed_hidden=[64],\n",
        "    #     uni_hidden=[256, 256],\n",
        "    #     warmup_steps=1000,\n",
        "    #     update_interval=4,\n",
        "    #     max_traj_len=600,\n",
        "    #     chunk_len=96,\n",
        "    #     burn_in=32\n",
        "    # )\n",
        "\n",
        "    agent = LSTMSAC(\n",
        "        env=env,\n",
        "        seed=42,\n",
        "        gamma=0.99,\n",
        "\n",
        "        # 1. Stability: Lower tau significantly for long runs\n",
        "        tau=0.002,           # (was 0.005) - Smooths target updates\n",
        "\n",
        "        # 2. Stability: Lower LR slightly\n",
        "        lr=1e-4,             # (Standard stable baseline, current was 1e-4 which is okay, but 3e-4 is often better for SAC if batch size is larger)\n",
        "                             # actually, keep 1e-4 if 3e-4 is unstable, but 1e-4 is safe.\n",
        "                             # RECOMMENDATION: Keep 1e-4 or try 3e-4 carefully. Let's stick to 1e-4 but increase batch size.\n",
        "\n",
        "        # 3. Batch Size: Increase for better gradient estimation\n",
        "        batch_size=256,      # (was 128) - Reduces variance\n",
        "\n",
        "        # 4. Network Capacity: Increase universal network size\n",
        "        embed_dim=64,        # (Keep as is)\n",
        "        embed_hidden=[64],   # (Keep as is)\n",
        "        uni_hidden=[512, 512], # (was [256, 256]) - Larger capacity for 400k steps\n",
        "\n",
        "        # 5. Temporal Memory: Longer chunks for better history learning\n",
        "        chunk_len=256,       # (was 96) - See more history\n",
        "        burn_in=40,          # (was 32) - Better hidden state initialization\n",
        "\n",
        "        warmup_steps=5000,   # (was 1000) - Gather more data before training starts\n",
        "        update_interval=2,   # (was 4) - Update more frequently now that we have more steps\n",
        "        max_traj_len=600,\n",
        "    )\n",
        "\n",
        "    rewards, lengths = agent.train(\n",
        "        total_steps=TOTAL_TIMESTEPS,\n",
        "        eval_interval=5000,\n",
        "        log_interval=1000\n",
        "    )\n",
        "\n",
        "    model_path = os.path.join(MODELS_DIR, EXPERIMENT_NAME)\n",
        "    agent.save(model_path)\n",
        "\n",
        "    csv_path = os.path.join(RESULTS_DIR, \"reward.csv\")\n",
        "    agent.save_rewards_csv(csv_path)\n",
        "\n",
        "    plot_path = os.path.join(RESULTS_DIR, f\"{EXPERIMENT_NAME}_training_curve.png\")\n",
        "    plot_training_reward_curve(\n",
        "        ep_rewards=rewards,\n",
        "        experiment_name=EXPERIMENT_NAME,\n",
        "        ep_lengths=lengths,\n",
        "        window_size=10,\n",
        "        save_path=plot_path\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal Evaluation:\")\n",
        "    final_reward = agent.evaluate(n_episodes=10)\n",
        "    print(f\"Mean Reward over 10 episodes: {final_reward:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZQM5CeVrpUT",
        "outputId": "aae535d9-0b83-4cfb-e604-8a82a23a0ae5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-10T00:02:42.974262Z",
          "iopub.execute_input": "2025-12-10T00:02:42.974554Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LSTM-SAC for Cancer Chemotherapy Environment\n",
            "(Using Repository RNNBase/ContextualModel Pattern with LSTM)\n",
            "============================================================\n",
            "Using device: cuda\n",
            "LSTM config: embed_dim=64, embed_hidden=[64]\n",
            "Starting LSTM-SAC training for 400000 steps...\n",
            "Step 5000/400000 | Eps: 8 | Reward: -13.7 | Alpha: 0.2000 | Time: 5s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "[EVAL] Step 5000 | Mean Reward: 65.88\n",
            "Model saved to ./models/best\n",
            "[BEST] New best model saved! Reward: 65.88\n",
            "Step 6000/400000 | Eps: 10 | Reward: -47.1 | Alpha: 0.1915 | Time: 141s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 7000/400000 | Eps: 13 | Reward: -175.3 | Alpha: 0.1831 | Time: 276s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 8000/400000 | Eps: 15 | Reward: -237.0 | Alpha: 0.1738 | Time: 412s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 9000/400000 | Eps: 18 | Reward: -387.9 | Alpha: 0.1648 | Time: 547s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 10000/400000 | Eps: 23 | Reward: -379.9 | Alpha: 0.1573 | Time: 682s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "[EVAL] Step 10000 | Mean Reward: -225.04\n",
            "Step 11000/400000 | Eps: 25 | Reward: -399.7 | Alpha: 0.1493 | Time: 818s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 12000/400000 | Eps: 29 | Reward: -370.2 | Alpha: 0.1424 | Time: 953s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 13000/400000 | Eps: 35 | Reward: -332.6 | Alpha: 0.1367 | Time: 1088s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 14000/400000 | Eps: 41 | Reward: -263.2 | Alpha: 0.1314 | Time: 1223s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 15000/400000 | Eps: 46 | Reward: -284.2 | Alpha: 0.1259 | Time: 1357s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "[EVAL] Step 15000 | Mean Reward: -190.69\n",
            "Step 16000/400000 | Eps: 51 | Reward: -306.3 | Alpha: 0.1205 | Time: 1493s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 17000/400000 | Eps: 55 | Reward: -294.1 | Alpha: 0.1200 | Time: 1627s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 18000/400000 | Eps: 64 | Reward: -227.6 | Alpha: 0.1200 | Time: 1761s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 19000/400000 | Eps: 74 | Reward: -194.6 | Alpha: 0.1200 | Time: 1895s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 20000/400000 | Eps: 84 | Reward: -194.0 | Alpha: 0.1200 | Time: 2029s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "[EVAL] Step 20000 | Mean Reward: -183.78\n",
            "Step 21000/400000 | Eps: 93 | Reward: -205.0 | Alpha: 0.1200 | Time: 2163s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 22000/400000 | Eps: 97 | Reward: -241.3 | Alpha: 0.1200 | Time: 2297s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 23000/400000 | Eps: 98 | Reward: -235.5 | Alpha: 0.1200 | Time: 2430s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 24000/400000 | Eps: 100 | Reward: -201.2 | Alpha: 0.1200 | Time: 2564s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 25000/400000 | Eps: 102 | Reward: -154.1 | Alpha: 0.1200 | Time: 2697s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "[EVAL] Step 25000 | Mean Reward: 114.62\n",
            "Model saved to ./models/best\n",
            "[BEST] New best model saved! Reward: 114.62\n",
            "Step 26000/400000 | Eps: 103 | Reward: -121.6 | Alpha: 0.1200 | Time: 2836s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 27000/400000 | Eps: 105 | Reward: -58.8 | Alpha: 0.1200 | Time: 2970s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 28000/400000 | Eps: 107 | Reward: 40.5 | Alpha: 0.1200 | Time: 3103s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 29000/400000 | Eps: 108 | Reward: 71.6 | Alpha: 0.1200 | Time: 3237s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 30000/400000 | Eps: 110 | Reward: 106.2 | Alpha: 0.1200 | Time: 3371s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "[EVAL] Step 30000 | Mean Reward: 171.31\n",
            "Model saved to ./models/best\n",
            "[BEST] New best model saved! Reward: 171.31\n",
            "Step 31000/400000 | Eps: 112 | Reward: 130.0 | Alpha: 0.1200 | Time: 3510s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 32000/400000 | Eps: 113 | Reward: 134.9 | Alpha: 0.1200 | Time: 3644s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 33000/400000 | Eps: 115 | Reward: 151.0 | Alpha: 0.1200 | Time: 3778s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 34000/400000 | Eps: 117 | Reward: 170.2 | Alpha: 0.1200 | Time: 3912s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 35000/400000 | Eps: 118 | Reward: 175.0 | Alpha: 0.1200 | Time: 4046s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "[EVAL] Step 35000 | Mean Reward: 193.71\n",
            "Model saved to ./models/best\n",
            "[BEST] New best model saved! Reward: 193.71\n",
            "Step 36000/400000 | Eps: 120 | Reward: 188.3 | Alpha: 0.1200 | Time: 4186s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 37000/400000 | Eps: 122 | Reward: 197.5 | Alpha: 0.1200 | Time: 4319s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 38000/400000 | Eps: 123 | Reward: 201.7 | Alpha: 0.1200 | Time: 4453s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 39000/400000 | Eps: 125 | Reward: 210.3 | Alpha: 0.1200 | Time: 4587s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 40000/400000 | Eps: 127 | Reward: 214.6 | Alpha: 0.1200 | Time: 4720s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "[EVAL] Step 40000 | Mean Reward: 263.64\n",
            "Model saved to ./models/best\n",
            "[BEST] New best model saved! Reward: 263.64\n",
            "Step 41000/400000 | Eps: 128 | Reward: 219.9 | Alpha: 0.1200 | Time: 4860s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 42000/400000 | Eps: 130 | Reward: 226.0 | Alpha: 0.1200 | Time: 4993s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 43000/400000 | Eps: 132 | Reward: 234.9 | Alpha: 0.1200 | Time: 5127s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 44000/400000 | Eps: 133 | Reward: 239.9 | Alpha: 0.1200 | Time: 5261s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 45000/400000 | Eps: 135 | Reward: 246.4 | Alpha: 0.1200 | Time: 5395s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "[EVAL] Step 45000 | Mean Reward: 274.77\n",
            "Model saved to ./models/best\n",
            "[BEST] New best model saved! Reward: 274.77\n",
            "Step 46000/400000 | Eps: 137 | Reward: 249.7 | Alpha: 0.1200 | Time: 5534s\n",
            "Rewards saved to: ./results/reward.csv\n",
            "Step 47000/400000 | Eps: 138 | Reward: 251.5 | Alpha: 0.1200 | Time: 5668s\n",
            "Rewards saved to: ./results/reward.csv\n"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}